# Azure OpenAI / Foundry LLM Sizing Guide (PTU vs Pay-as-you-Go)

> **Audience**: Enterprise architects, platform engineers, and cloud capacity planners  
> **Scope**: Azure OpenAI / Foundry LLM deployments using **Provisioned Throughput Units (PTUs)** and **Payâ€‘Asâ€‘Youâ€‘Go (PAYG)**  
> **Models**: GPTâ€‘4o, GPTâ€‘5 family (GPTâ€‘5, GPTâ€‘5.1, GPTâ€‘5 mini)

***

## 1. What a PTU Really Is (and What It Is Not)

A **Provisioned Throughput Unit (PTU)** represents **reserved, dedicated model-side compute capacity**.

âœ… PTU guarantees:

*   Sustained throughput
*   Predictable latency
*   Isolation from noisy neighbors
*   No throttling up to allocated capacity

âŒ PTU is **not**:

*   A fixed token quota
*   A prepaid token bucket
*   A guaranteed monthly token allowance

**Key principle**

> PTUs reserve *capacity* (tokens / second), not *usage* (tokens / month).

Unused PTU capacity is **not carried forward**.

***

## 2. Baseline: Tokens per Second per PTU

Microsoft publishes exact throughput mappings **only for GPTâ€‘4o today**.  
For newer Foundry models, the numbers below are **planning guidance**, not contractual SLAs.

### GPTâ€‘4o (Reference Baseline)

```text
1 PTU â‰ˆ 30 tokens / second (input + output combined)
```

This value is widely validated in production Azure OpenAI PTU deployments and is used as the **normalization anchor** for all other models.

***

## 3. PTU Planning Ranges by Model (Foundry)

| Model          | Tokens/sec per PTU (planning) | Relative to GPTâ€‘4o | Profile                           |
| -------------- | ----------------------------- | ------------------ | --------------------------------- |
| **GPTâ€‘4o**     | \~30                          | 1.0Ã—               | Multimodal, realâ€‘time             |
| **GPTâ€‘5**      | \~10â€“15                       | \~0.3â€“0.5Ã—         | Frontier reasoning, heavy compute |
| **GPTâ€‘5.1**    | \~18â€“25                       | \~0.6â€“0.8Ã—         | Balanced flagship                 |
| **GPTâ€‘5 mini** | \~45â€“70                       | \~1.5â€“2.3Ã—         | Highâ€‘throughput, costâ€‘efficient   |

> âš ï¸ Exact throughput varies with:
>
> *   Input vs output ratio
> *   Context window size
> *   Tool / function calling
> *   Multimodal inputs

Always size with **30â€“40% headroom**.

***

## 4. Translating PTUs â†’ Tokens Over Time

### Example: GPTâ€‘4o

```text
1 PTU = 30 tokens/sec
100 PTU = 3,000 tokens/sec
```

| Time Window            | Tokens @ 100 PTU |
| ---------------------- | ---------------- |
| Per second             | 3,000            |
| Per minute             | 180,000          |
| Per hour               | 10.8 million     |
| Per day (24h)          | 259 million      |
| Per 30â€‘day month (max) | \~7.8 billion    |

> This is a **theoretical maximum** assuming **24Ã—7 sustained utilization**.

### Realistic Utilization Scenarios

| Pattern                  | Estimated Monthly Tokens (100 PTU) |
| ------------------------ | ---------------------------------- |
| 24Ã—7 at 100%             | \~7.8B                             |
| 50% average load         | \~3.9B                             |
| Business hours (10h/day) | \~3.2B                             |
| Eventâ€‘driven / spiky     | \~1.5â€“3B                           |

***

## 5. Input vs Output Token Considerations

Typical enterprise workloads are **not 50/50**.

| Use Case          | Input  | Output |
| ----------------- | ------ | ------ |
| Chat / RAG        | 60â€“70% | 30â€“40% |
| Summarization     | 40â€“50% | 50â€“60% |
| Agentic workflows | 70â€“80% | 20â€“30% |

> PTU throughput counts **combined input + output tokens**.

***

## 6. How to Size PTUs Correctly (Recommended Method)

### Stepâ€‘byâ€‘step formula

```text
PTUs required =
(RPS Ã— Avg tokens per request)
Ã·
(Tokens/sec per PTU for the model)
```

### Add safety margins

*   +30â€“40% capacity headroom
*   Separate PTUs per critical workload tier (do not share blindly)

### Common enterprise pattern

| Tier       | Model      | Reason                          |
| ---------- | ---------- | ------------------------------- |
| Tier 1     | GPTâ€‘5      | Critical reasoning, decisions   |
| Tier 2     | GPTâ€‘5.1    | General assistant               |
| Tier 3     | GPTâ€‘5 mini | RAG, summarization, chat volume |
| Multimodal | GPTâ€‘4o     | Voice, vision, realâ€‘time        |

***

## 7. Translating PTU Sizing to Payâ€‘Asâ€‘Youâ€‘Go (PAYG)

### Key Differences

| Dimension           | PTU                         | Payâ€‘Asâ€‘Youâ€‘Go        |
| ------------------- | --------------------------- | -------------------- |
| Billing             | Reserved capacity           | Tokens used          |
| Latency             | Stable                      | Bestâ€‘effort          |
| Throttling          | None within allocation      | Possible             |
| Best for            | Regulated, high concurrency | Bursty, low baseline |
| Cost predictability | High                        | Usageâ€‘dependent      |

***

Perfect catch ðŸ‘ â€” youâ€™re absolutely right.  
For **PAYG**, the **enforced limit and operational control is TPM (Tokens Per Minute)**, *not* monthly totals.

Below is a **surgically corrected replacement for Point 8**, written so it can drop straight into your guide and be used as a **sizing + governance reference**.

Iâ€™ll **only rewrite Point 8**, keeping everything else intact.

***

## 8. Translating PTU Sizing to Payâ€‘Asâ€‘Youâ€‘Go (TPMâ€‘Based)

In **Payâ€‘Asâ€‘Youâ€‘Go (PAYG)** deployments, Azure enforces limits using **Tokens Per Minute (TPM)** and **Requests Per Minute (RPM)** â€” **not monthly token quotas**.

### Key Mental Model

> **PTU = guaranteed tokens per second**  
> **PAYG = capped tokens per minute (*TPM*)**

To translate PTU capacity into PAYG limits, **convert throughput into TPM**.

***

### 8.1 PTU â†’ TPM Conversion Formula

```text
TPM â‰ˆ Tokens per second Ã— 60
```

***

### 8.2 Reference Conversion (GPTâ€‘4o)

Baseline (validated):

```text
1 PTU â‰ˆ 30 tokens / second
```

Therefore:

```text
1 PTU â‰ˆ 1,800 TPM
```

***

### 8.3 Common Reference Table (GPTâ€‘4o)

| PTUs | Tokens/sec | TPM     |
| ---- | ---------- | ------- |
| 1    | 30         | 1,800   |
| 10   | 300        | 18,000  |
| 25   | 750        | 45,000  |
| 50   | 1,500      | 90,000  |
| 100  | 3,000      | 180,000 |

âœ… **180k TPM** is the PAYG equivalent of **100 PTU GPTâ€‘4o sustained capacity**.

***

### 8.4 PAYG TPM Planning for Foundry Models

Using the planning throughput ranges:

| Model      | Tokens/sec per PTU | TPM per PTU   |
| ---------- | ------------------ | ------------- |
| GPTâ€‘4o     | \~30               | \~1,800       |
| GPTâ€‘5      | \~10â€“15            | \~600â€“900     |
| GPTâ€‘5.1    | \~18â€“25            | \~1,080â€“1,500 |
| GPTâ€‘5 mini | \~45â€“70            | \~2,700â€“4,200 |

âš ï¸ These values are **planning guidance**, not hard guarantees.

***

### 8.5 Practical PAYG Limit Setting Strategy

When configuring PAYG:

1.  **Set TPM slightly below theoretical maximum**
    *   (e.g., 70â€“85%) to avoid burst throttling
2.  **Set RPM independently**
    *   Avoid many small requests overwhelming TPM
3.  **Split TPM per workload**
    *   Frontend chat
    *   Background RAG
    *   Batch summarization

***

### 8.6 Example: PTU â†’ PAYG Decision Comparison

**Scenario**:  
A workload sized at **100 PTU GPTâ€‘4o**

Equivalent PAYG configuration:

```text
Target TPM: 150kâ€“180k
RPM: sized separately based on request profile
```

| Dimension              | PTU                         | PAYG            |
| ---------------------- | --------------------------- | --------------- |
| Throughput guarantee   | Yes                         | No              |
| Enforced limit         | Tokens/sec                  | TPM             |
| Latency predictability | High                        | Variable        |
| Best for               | Regulated, high concurrency | Bursty, elastic |

***

### 8.7 Final Decision Rule (Enterpriseâ€‘Friendly)

> **If you design in PTU, validate in TPM.**  
> **If you deploy PAYG, enforce in TPM.**

This ensures:

*   Correct throttling expectations
*   Clean migration path PTU â†” PAYG
*   No surprises during peak load


## 9. When PTU Makes Financial Sense

âœ… PTU is usually justified when:

*   Consistent traffic â‰¥ 30â€“40% utilization
*   Latency SLOs are strict
*   Throttling is unacceptable
*   Regulated / banking workloads
*   Large user concurrency

âœ… PAYG is better when:

*   Traffic is bursty
*   Low baseline usage
*   Nonâ€‘critical workloads
*   Early experimentation

***

## 10. Practical Rule of Thumb

> **Size with PTU for concurrency and latency.  
> Size with PAYG for exploration and variability.**

Many production platforms use **both** together and leverage AI Gateway to manage and optimize their AI workloads effectively.

***