<policies>
    <inbound>
        <base />
        
        <!--
            Universal LLM API Policy
            Purpose: Routes requests to appropriate backend pools based on model selection and RBAC permissions
            
            Flow:
            1. Extract and validate model parameter from request payload
            2. Configure backend pools and routing rules
            3. Determine target backend pool based on model and permissions
            4. Set up authentication and route to selected backend
            5. Configure collecting usage metrics
            
            Configuration:
            - Modify allowedBackendPools for RBAC (comma-separated pool IDs, empty = all allowed)
            - Set defaultBackendPool for unmapped models (empty = return error)
            - Backend pool definitions are in frag-set-backend-pools fragment
        -->
        
        <!-- Step 1: Extract and validate model parameter from request -->
        <include-fragment fragment-id="set-llm-requested-model" />
        
        <!-- Remove api-key header to prevent it from being passed to backend endpoints -->
        <set-header name="api-key" exists-action="delete" />
        
        <!-- Step 2: Configure RBAC and default routing behavior -->
        <!-- RBAC: Set allowed backend pools (comma-separated pool IDs, empty = all pools allowed) -->
        <set-variable name="allowedBackendPools" value="" />
        
        <!-- Set default backend pool (empty = return error for unmapped models) -->
        <set-variable name="defaultBackendPool" value="" />

        <!-- Step 3: Load backend pool configurations -->
        <include-fragment fragment-id="set-backend-pools" />

        <!-- Step 4: Determine target backend pool based on model and permissions -->
        <include-fragment fragment-id="set-target-backend-pool" />

        <!-- Step 5: Configure authentication and route to selected backend -->
        <include-fragment fragment-id="set-backend-authorization" />

        <!-- Step 6: Configure collecting usage metrics -->
        <include-fragment fragment-id="set-llm-usage" />
    </inbound>
    <backend>
        <!--
            Backend Retry Logic
            Purpose: Implements retry mechanism for transient failures (429 throttling, 503 service unavailable)
            
            Configuration:
            - Retry count: Set to one less than number of backends in pool to try all backends
            - Condition: Retries on 429 (throttling) or 503 (except when backend pool is unavailable)
            - Strategy: First fast retry with zero interval, buffer request body for replay
        -->
        <retry count="2" interval="0" first-fast-retry="true" condition="@(context.Response.StatusCode == 429 || (context.Response.StatusCode >= 500 &amp;&amp; !context.Response.StatusReason.Contains(&quot;Backend pool&quot;) &amp;&amp; !context.Response.StatusReason.Contains(&quot;is temporarily unavailable&quot;)))">
            <forward-request buffer-request-body="true" />
        </retry>
    </backend>
    <outbound>
        <base />
    </outbound>
    <on-error>
        <base />
        <!--
            Error Handling
            Purpose: Logs error information for monitoring and debugging
        -->
        <set-variable name="service-name" value="Universal LLM API" />
        <set-variable name="target-deployment" value="@((string)context.Variables.GetValueOrDefault(&quot;requestedModel&quot;, &quot;unknown&quot;))" />
    </on-error>
</policies>
