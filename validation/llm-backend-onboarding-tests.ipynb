{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edacfa1c",
   "metadata": {},
   "source": [
    "# LLM Backend Onboarding Tests\n",
    "\n",
    "This notebook provides comprehensive testing for LLM backend onboarding deployments. It validates:\n",
    "- Backend configuration and connectivity\n",
    "- Load balancing across backend pools\n",
    "- Circuit breaker functionality\n",
    "- Response latency and performance\n",
    "\n",
    "## Prerequisites\n",
    "- Azure subscription with deployed APIM instance\n",
    "- LLM backends onboarded via the `llm-backend-onboarding` Bicep deployment\n",
    "- Python 3.11+ with required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install azure-identity azure-mgmt-apimanagement openai requests pandas matplotlib httpx tenacity --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "import httpx\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.mgmt.apimanagement import ApiManagementClient\n",
    "from openai import AzureOpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "# Configuration - Update these values\n",
    "SUBSCRIPTION_ID = os.environ.get(\"AZURE_SUBSCRIPTION_ID\", \"<your-subscription-id>\")\n",
    "RESOURCE_GROUP = os.environ.get(\"APIM_RESOURCE_GROUP\", \"<your-rg>\")\n",
    "APIM_NAME = os.environ.get(\"APIM_NAME\", \"<your-apim-name>\")\n",
    "APIM_GATEWAY_URL = os.environ.get(\"APIM_GATEWAY_URL\", f\"https://{APIM_NAME}.azure-api.net\")\n",
    "APIM_SUBSCRIPTION_KEY = os.environ.get(\"APIM_SUBSCRIPTION_KEY\", \"<your-subscription-key>\")\n",
    "\n",
    "# Initialize Azure credentials\n",
    "credential = DefaultAzureCredential()\n",
    "print(f\"Configuration loaded for APIM: {APIM_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226bd4b",
   "metadata": {},
   "source": [
    "## 1. Verify LLM Backend Configuration\n",
    "\n",
    "Retrieve and validate the current LLM backend configuration from APIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize APIM Management Client\n",
    "apim_client = ApiManagementClient(credential, SUBSCRIPTION_ID)\n",
    "\n",
    "def get_llm_backends():\n",
    "    \"\"\"Retrieve all LLM backends from APIM.\"\"\"\n",
    "    backends = list(apim_client.backend.list_by_service(RESOURCE_GROUP, APIM_NAME))\n",
    "    llm_backends = [b for b in backends if b.name and b.name.startswith(\"llm-\")]\n",
    "    return llm_backends\n",
    "\n",
    "def get_backend_pools():\n",
    "    \"\"\"Retrieve all backend pools from APIM.\"\"\"\n",
    "    pools = list(apim_client.backend.list_by_service(RESOURCE_GROUP, APIM_NAME))\n",
    "    backend_pools = [p for p in pools if p.pool and p.pool.services]\n",
    "    return backend_pools\n",
    "\n",
    "def get_policy_fragments():\n",
    "    \"\"\"Retrieve LLM-related policy fragments.\"\"\"\n",
    "    fragments = list(apim_client.policy_fragment.list_by_service(RESOURCE_GROUP, APIM_NAME))\n",
    "    llm_fragments = [f for f in fragments if f.name and \"llm\" in f.name.lower()]\n",
    "    return llm_fragments\n",
    "\n",
    "# Display backend configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"LLM BACKEND CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "backends = get_llm_backends()\n",
    "print(f\"\\nüì¶ Found {len(backends)} LLM backends:\")\n",
    "for b in backends:\n",
    "    print(f\"  - {b.name}: {b.url}\")\n",
    "    if hasattr(b, 'circuit_breaker') and b.circuit_breaker:\n",
    "        print(f\"    Circuit Breaker: enabled\")\n",
    "\n",
    "pools = get_backend_pools()\n",
    "print(f\"\\nüîÑ Found {len(pools)} backend pools:\")\n",
    "for p in pools:\n",
    "    if p.pool and p.pool.services:\n",
    "        print(f\"  - {p.name}:\")\n",
    "        for svc in p.pool.services:\n",
    "            print(f\"    ‚Ä¢ {svc.id} (priority: {getattr(svc, 'priority', 'N/A')}, weight: {getattr(svc, 'weight', 'N/A')})\")\n",
    "\n",
    "fragments = get_policy_fragments()\n",
    "print(f\"\\nüìã Found {len(fragments)} LLM policy fragments:\")\n",
    "for f in fragments:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379dbb4",
   "metadata": {},
   "source": [
    "## 2. Test LLM API Connectivity\n",
    "\n",
    "Test basic connectivity to the Universal LLM API through APIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API connectivity\n",
    "INFERENCE_API_PATH = \"/inference\"  # Update based on your deployment\n",
    "\n",
    "def test_chat_completion(model: str, prompt: str = \"Hello, how are you?\") -> dict:\n",
    "    \"\"\"Send a chat completion request through APIM.\"\"\"\n",
    "    url = f\"{APIM_GATEWAY_URL}{INFERENCE_API_PATH}/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Ocp-Apim-Subscription-Key\": APIM_SUBSCRIPTION_KEY,\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"status_code\": response.status_code,\n",
    "        \"elapsed_ms\": round(elapsed * 1000, 2),\n",
    "        \"response\": response.json() if response.status_code == 200 else response.text,\n",
    "        \"headers\": dict(response.headers)\n",
    "    }\n",
    "\n",
    "# Test with a sample model\n",
    "TEST_MODEL = \"gpt-4o\"  # Update based on your deployed models\n",
    "\n",
    "print(f\"Testing chat completion with model: {TEST_MODEL}\")\n",
    "result = test_chat_completion(TEST_MODEL)\n",
    "\n",
    "print(f\"\\n‚úÖ Status: {result['status_code']}\")\n",
    "print(f\"‚è±Ô∏è  Latency: {result['elapsed_ms']} ms\")\n",
    "\n",
    "if result['status_code'] == 200:\n",
    "    response_content = result['response'].get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "    print(f\"üí¨ Response: {response_content[:200]}...\")\n",
    "    \n",
    "    # Check for backend info in headers\n",
    "    if 'x-backend-id' in result['headers']:\n",
    "        print(f\"üéØ Backend: {result['headers']['x-backend-id']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935b8a2",
   "metadata": {},
   "source": [
    "## 3. Test Load Balancing Across Backend Pools\n",
    "\n",
    "Send multiple requests to verify load distribution across backends in a pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d562ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_concurrent_requests(model: str, num_requests: int = 20) -> list:\n",
    "    \"\"\"Send concurrent requests to test load balancing.\"\"\"\n",
    "    url = f\"{APIM_GATEWAY_URL}{INFERENCE_API_PATH}/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Ocp-Apim-Subscription-Key\": APIM_SUBSCRIPTION_KEY,\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Say 'test' and nothing else.\"}],\n",
    "        \"max_tokens\": 10\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        async def send_request(i: int):\n",
    "            start = time.time()\n",
    "            try:\n",
    "                response = await client.post(url, headers=headers, json=payload)\n",
    "                elapsed = time.time() - start\n",
    "                return {\n",
    "                    \"request_id\": i,\n",
    "                    \"status\": response.status_code,\n",
    "                    \"latency_ms\": round(elapsed * 1000, 2),\n",
    "                    \"backend\": response.headers.get(\"x-backend-id\", \"unknown\"),\n",
    "                    \"success\": response.status_code == 200\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"request_id\": i,\n",
    "                    \"status\": 0,\n",
    "                    \"latency_ms\": round((time.time() - start) * 1000, 2),\n",
    "                    \"backend\": \"error\",\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "        \n",
    "        tasks = [send_request(i) for i in range(num_requests)]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run load balancing test\n",
    "print(f\"Sending 20 concurrent requests to model: {TEST_MODEL}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = await send_concurrent_requests(TEST_MODEL, 20)\n",
    "\n",
    "# Analyze results\n",
    "df = pd.DataFrame(results)\n",
    "success_count = df['success'].sum()\n",
    "print(f\"\\n‚úÖ Successful requests: {success_count}/{len(results)}\")\n",
    "print(f\"‚è±Ô∏è  Average latency: {df['latency_ms'].mean():.2f} ms\")\n",
    "print(f\"‚è±Ô∏è  Min latency: {df['latency_ms'].min():.2f} ms\")\n",
    "print(f\"‚è±Ô∏è  Max latency: {df['latency_ms'].max():.2f} ms\")\n",
    "\n",
    "# Backend distribution\n",
    "print(\"\\nüìä Backend Distribution:\")\n",
    "backend_counts = df['backend'].value_counts()\n",
    "for backend, count in backend_counts.items():\n",
    "    pct = (count / len(results)) * 100\n",
    "    print(f\"  {backend}: {count} requests ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a5641",
   "metadata": {},
   "source": [
    "## 4. Visualize Backend Distribution\n",
    "\n",
    "Create charts showing request distribution and latency across backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart - Request distribution\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.Pastel1.colors\n",
    "ax1.pie(backend_counts.values, labels=backend_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "ax1.set_title('Request Distribution Across Backends')\n",
    "\n",
    "# Box plot - Latency by backend\n",
    "ax2 = axes[1]\n",
    "df_success = df[df['success']]\n",
    "if len(df_success) > 0:\n",
    "    backends = df_success['backend'].unique()\n",
    "    latency_data = [df_success[df_success['backend'] == b]['latency_ms'].values for b in backends]\n",
    "    bp = ax2.boxplot(latency_data, labels=backends, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    ax2.set_ylabel('Latency (ms)')\n",
    "    ax2.set_title('Response Latency by Backend')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Latency Statistics by Backend:\")\n",
    "for backend in backends:\n",
    "    backend_df = df_success[df_success['backend'] == backend]\n",
    "    print(f\"\\n  {backend}:\")\n",
    "    print(f\"    Mean: {backend_df['latency_ms'].mean():.2f} ms\")\n",
    "    print(f\"    Std:  {backend_df['latency_ms'].std():.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbbab5",
   "metadata": {},
   "source": [
    "## 5. Test Circuit Breaker Failover\n",
    "\n",
    "Simulate backend failures to verify circuit breaker behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_failover_behavior(model: str, num_requests: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Test circuit breaker failover by monitoring backend switching.\n",
    "    \n",
    "    Note: To truly test failover, you would need to:\n",
    "    1. Disable a backend in APIM\n",
    "    2. Observe requests redirecting to healthy backends\n",
    "    3. Re-enable the backend\n",
    "    \"\"\"\n",
    "    url = f\"{APIM_GATEWAY_URL}{INFERENCE_API_PATH}/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Ocp-Apim-Subscription-Key\": APIM_SUBSCRIPTION_KEY,\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}],\n",
    "        \"max_tokens\": 5\n",
    "    }\n",
    "    \n",
    "    timeline = []\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            timeline.append({\n",
    "                \"request_num\": i + 1,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"status\": response.status_code,\n",
    "                \"backend\": response.headers.get(\"x-backend-id\", \"unknown\"),\n",
    "                \"latency_ms\": round(elapsed * 1000, 2),\n",
    "                \"success\": response.status_code == 200\n",
    "            })\n",
    "        except Exception as e:\n",
    "            timeline.append({\n",
    "                \"request_num\": i + 1,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"status\": 0,\n",
    "                \"backend\": \"error\",\n",
    "                \"latency_ms\": 0,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "        \n",
    "        # Small delay between requests\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return timeline\n",
    "\n",
    "print(\"Testing failover behavior (sequential requests with delays)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "timeline = test_failover_behavior(TEST_MODEL, 10)\n",
    "\n",
    "# Display timeline\n",
    "print(\"\\nüìä Request Timeline:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'#':<4} {'Backend':<30} {'Status':<8} {'Latency':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for entry in timeline:\n",
    "    status_icon = \"‚úÖ\" if entry['success'] else \"‚ùå\"\n",
    "    print(f\"{entry['request_num']:<4} {entry['backend']:<30} {status_icon} {entry['status']:<5} {entry['latency_ms']:<10} ms\")\n",
    "\n",
    "# Check for backend switches\n",
    "backends_used = [e['backend'] for e in timeline if e['success']]\n",
    "unique_backends = set(backends_used)\n",
    "print(f\"\\nüîÑ Backends used: {', '.join(unique_backends)}\")\n",
    "print(f\"üìà Backend switches detected: {sum(1 for i in range(1, len(backends_used)) if backends_used[i] != backends_used[i-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656cd67",
   "metadata": {},
   "source": [
    "## 6. Test Multiple Models\n",
    "\n",
    "Verify that different models are routed to appropriate backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test - Update based on your deployment\n",
    "MODELS_TO_TEST = [\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\", \n",
    "    \"text-embedding-ada-002\"\n",
    "]\n",
    "\n",
    "def test_model_routing(models: list) -> dict:\n",
    "    \"\"\"Test that each model routes to appropriate backends.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nüîç Testing model: {model}\")\n",
    "        \n",
    "        url = f\"{APIM_GATEWAY_URL}{INFERENCE_API_PATH}/chat/completions\"\n",
    "        \n",
    "        # Use embeddings endpoint for embedding models\n",
    "        if \"embedding\" in model.lower():\n",
    "            url = f\"{APIM_GATEWAY_URL}{INFERENCE_API_PATH}/embeddings\"\n",
    "            payload = {\n",
    "                \"model\": model,\n",
    "                \"input\": \"Test embedding\"\n",
    "            }\n",
    "        else:\n",
    "            payload = {\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                \"max_tokens\": 10\n",
    "            }\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Ocp-Apim-Subscription-Key\": APIM_SUBSCRIPTION_KEY,\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            start = time.time()\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            results[model] = {\n",
    "                \"status\": response.status_code,\n",
    "                \"backend\": response.headers.get(\"x-backend-id\", \"unknown\"),\n",
    "                \"latency_ms\": round(elapsed * 1000, 2),\n",
    "                \"success\": response.status_code == 200\n",
    "            }\n",
    "            \n",
    "            status_icon = \"‚úÖ\" if response.status_code == 200 else \"‚ùå\"\n",
    "            print(f\"  {status_icon} Status: {response.status_code}\")\n",
    "            print(f\"  üéØ Backend: {results[model]['backend']}\")\n",
    "            print(f\"  ‚è±Ô∏è  Latency: {results[model]['latency_ms']} ms\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[model] = {\n",
    "                \"status\": 0,\n",
    "                \"backend\": \"error\",\n",
    "                \"latency_ms\": 0,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            print(f\"  ‚ùå Error: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Testing model routing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_results = test_model_routing(MODELS_TO_TEST)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL ROUTING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_df = pd.DataFrame.from_dict(model_results, orient='index')\n",
    "summary_df.index.name = 'Model'\n",
    "print(summary_df[['status', 'backend', 'latency_ms', 'success']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66162c4",
   "metadata": {},
   "source": [
    "## 7. Test Using OpenAI SDK\n",
    "\n",
    "Verify compatibility with the official OpenAI Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74569be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with OpenAI SDK\n",
    "def test_with_openai_sdk():\n",
    "    \"\"\"Test LLM API using the official OpenAI Python SDK.\"\"\"\n",
    "    \n",
    "    # Configure client for APIM endpoint\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=APIM_GATEWAY_URL,\n",
    "        api_key=APIM_SUBSCRIPTION_KEY,\n",
    "        api_version=\"2024-10-21\"\n",
    "    )\n",
    "    \n",
    "    print(\"Testing with OpenAI SDK...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        start = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=TEST_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"What is 2 + 2? Answer with just the number.\"}\n",
    "            ],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"‚úÖ Success!\")\n",
    "        print(f\"üìù Model: {response.model}\")\n",
    "        print(f\"üí¨ Response: {response.choices[0].message.content}\")\n",
    "        print(f\"üî¢ Tokens: {response.usage.total_tokens}\")\n",
    "        print(f\"‚è±Ô∏è  Latency: {round(elapsed * 1000, 2)} ms\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "sdk_success = test_with_openai_sdk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102bd432",
   "metadata": {},
   "source": [
    "## 8. Latency Comparison Test\n",
    "\n",
    "Run extended tests to measure latency distribution across all backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extended_latency_test(model: str, num_requests: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"Run extended latency tests.\"\"\"\n",
    "    print(f\"Running extended latency test ({num_requests} requests)...\")\n",
    "    \n",
    "    results = await send_concurrent_requests(model, num_requests)\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run extended test\n",
    "latency_df = await extended_latency_test(TEST_MODEL, 50)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Latency histogram\n",
    "ax1 = axes[0, 0]\n",
    "latency_df[latency_df['success']]['latency_ms'].hist(bins=20, ax=ax1, color='steelblue', edgecolor='white')\n",
    "ax1.set_xlabel('Latency (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Latency Distribution')\n",
    "ax1.axvline(latency_df['latency_ms'].mean(), color='red', linestyle='--', label=f\"Mean: {latency_df['latency_ms'].mean():.0f}ms\")\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Latency over time\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(latency_df['request_id'], latency_df['latency_ms'], marker='o', markersize=4, linestyle='-', alpha=0.7)\n",
    "ax2.set_xlabel('Request #')\n",
    "ax2.set_ylabel('Latency (ms)')\n",
    "ax2.set_title('Latency Over Time')\n",
    "\n",
    "# 3. Success rate by backend\n",
    "ax3 = axes[1, 0]\n",
    "success_by_backend = latency_df.groupby('backend')['success'].mean() * 100\n",
    "success_by_backend.plot(kind='bar', ax=ax3, color='green', edgecolor='white')\n",
    "ax3.set_ylabel('Success Rate (%)')\n",
    "ax3.set_title('Success Rate by Backend')\n",
    "ax3.set_ylim(0, 105)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Request count by backend\n",
    "ax4 = axes[1, 1]\n",
    "backend_counts = latency_df['backend'].value_counts()\n",
    "backend_counts.plot(kind='bar', ax=ax4, color='orange', edgecolor='white')\n",
    "ax4.set_ylabel('Request Count')\n",
    "ax4.set_title('Request Distribution by Backend')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXTENDED LATENCY TEST SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal Requests: {len(latency_df)}\")\n",
    "print(f\"Successful: {latency_df['success'].sum()} ({latency_df['success'].mean()*100:.1f}%)\")\n",
    "print(f\"\\nLatency Percentiles:\")\n",
    "print(f\"  P50: {latency_df['latency_ms'].quantile(0.50):.0f} ms\")\n",
    "print(f\"  P90: {latency_df['latency_ms'].quantile(0.90):.0f} ms\")\n",
    "print(f\"  P95: {latency_df['latency_ms'].quantile(0.95):.0f} ms\")\n",
    "print(f\"  P99: {latency_df['latency_ms'].quantile(0.99):.0f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a9877",
   "metadata": {},
   "source": [
    "## 9. Cleanup Test Resources\n",
    "\n",
    "Optional cleanup for test resources created during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup function for LLM backend onboarding resources\n",
    "def cleanup_llm_backends(prefix: str = \"llm-\", dry_run: bool = True):\n",
    "    \"\"\"\n",
    "    Remove LLM backend resources from APIM.\n",
    "    \n",
    "    Args:\n",
    "        prefix: Prefix to identify LLM backend resources\n",
    "        dry_run: If True, only list resources without deleting\n",
    "    \"\"\"\n",
    "    print(f\"{'DRY RUN - ' if dry_run else ''}Cleaning up LLM backend resources...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # List backends to delete\n",
    "    backends = get_llm_backends()\n",
    "    print(f\"\\nüì¶ Found {len(backends)} LLM backends to remove:\")\n",
    "    for b in backends:\n",
    "        print(f\"  - {b.name}\")\n",
    "        if not dry_run:\n",
    "            try:\n",
    "                apim_client.backend.delete(RESOURCE_GROUP, APIM_NAME, b.name, if_match=\"*\")\n",
    "                print(f\"    ‚úÖ Deleted\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error: {e}\")\n",
    "    \n",
    "    # List policy fragments to delete\n",
    "    fragments = get_policy_fragments()\n",
    "    print(f\"\\nüìã Found {len(fragments)} LLM policy fragments to remove:\")\n",
    "    for f in fragments:\n",
    "        print(f\"  - {f.name}\")\n",
    "        if not dry_run:\n",
    "            try:\n",
    "                apim_client.policy_fragment.delete(RESOURCE_GROUP, APIM_NAME, f.name, if_match=\"*\")\n",
    "                print(f\"    ‚úÖ Deleted\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error: {e}\")\n",
    "    \n",
    "    if dry_run:\n",
    "        print(\"\\n‚ö†Ô∏è  DRY RUN - No resources were deleted.\")\n",
    "        print(\"   Set dry_run=False to actually delete resources.\")\n",
    "\n",
    "# Run cleanup in dry run mode first\n",
    "cleanup_llm_backends(dry_run=True)\n",
    "\n",
    "# Uncomment the following line to actually delete resources:\n",
    "# cleanup_llm_backends(dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bcb02c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook validated:\n",
    "- ‚úÖ LLM backend configuration retrieval\n",
    "- ‚úÖ API connectivity through APIM\n",
    "- ‚úÖ Load balancing across backend pools\n",
    "- ‚úÖ Backend distribution visualization\n",
    "- ‚úÖ Circuit breaker failover behavior\n",
    "- ‚úÖ Multi-model routing\n",
    "- ‚úÖ OpenAI SDK compatibility\n",
    "- ‚úÖ Latency performance metrics\n",
    "\n",
    "For more information, see the [LLM Backend Onboarding Guide](../guides/LLM-Backend-Onboarding-Guide.md)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
