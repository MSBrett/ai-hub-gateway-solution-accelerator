{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a5052f",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0Ô∏è‚É£ Initialize Notebook Variables\n",
    "\n",
    "Configure the following variables according to your environment before running the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys, json, requests, time\n",
    "sys.path.insert(1, '../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "from apimtools import APIMClientTool\n",
    "\n",
    "inference_api_version = \"2024-05-01-preview\"\n",
    "\n",
    "# ============================================================================\n",
    "# REQUIRED: Update these values for your environment\n",
    "# ============================================================================\n",
    "governance_hub_resource_group = \"REPLACE\"  ## specify the resource group name where the Governance Hub is located\n",
    "location = \"REPLACE\"  ## e.g., \"eastus\", \"westus2\", etc.\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: LLM Backend Configuration (pre-configured with sample values from main.bicepparam)\n",
    "# These values match the AI Foundry backends defined in the template\n",
    "# ============================================================================\n",
    "llm_backends_config = [\n",
    "    {\n",
    "        \"backendId\": \"aif-citadel-primary\",\n",
    "        \"backendType\": \"ai-foundry\",\n",
    "        \"endpoint\": \"https://REPLACE-0.services.ai.azure.com/models\",  # Replace RESOURCE_TOKEN\n",
    "        \"authScheme\": \"managedIdentity\",\n",
    "        \"supportedModels\": [\"gpt-4o\", \"gpt-4o-mini\", \"DeepSeek-R1\", \"Phi-4\"],\n",
    "        \"priority\": 1,\n",
    "        \"weight\": 100\n",
    "    },\n",
    "    {\n",
    "        \"backendId\": \"aif-citadel-secondary\",\n",
    "        \"backendType\": \"ai-foundry\",\n",
    "        \"endpoint\": \"https://REPLACE-1.services.ai.azure.com/models\",  # Replace RESOURCE_TOKEN\n",
    "        \"authScheme\": \"managedIdentity\",\n",
    "        \"supportedModels\": [\"gpt-5\", \"DeepSeek-R1\"],\n",
    "        \"priority\": 2,\n",
    "        \"weight\": 50\n",
    "    }\n",
    "]\n",
    "\n",
    "# Managed Identity for APIM authentication (will be auto-discovered if not specified)\n",
    "apim_managed_identity_name = \"\"  # Leave empty to auto-discover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7c8fd",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1Ô∏è‚É£ Verify Azure CLI and Connected Subscription\n",
    "\n",
    "Ensure Azure CLI is authenticated and connected to the correct subscription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f655ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = utils.run(\"az account show\", \"Retrieved az account\", \"Failed to get the current az account\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    current_user = output.json_data['user']['name']\n",
    "    tenant_id = output.json_data['tenantId']\n",
    "    subscription_id = output.json_data['id']\n",
    "\n",
    "    utils.print_info(f\"Current user: {current_user}\")\n",
    "    utils.print_info(f\"Tenant ID: {tenant_id}\")\n",
    "    utils.print_info(f\"Subscription ID: {subscription_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac82f7",
   "metadata": {},
   "source": [
    "<a id='init'></a>\n",
    "### ‚öôÔ∏è Initialize APIM Client Tool\n",
    "\n",
    "üëâ Initialize the APIM client to interact with your existing Governance Hub deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    apimClientTool = APIMClientTool(\n",
    "        governance_hub_resource_group\n",
    "    )\n",
    "    apimClientTool.initialize()\n",
    "    \n",
    "    apim_resource_name = apimClientTool.apim_resource_name\n",
    "    apim_resource_gateway_url = str(apimClientTool.apim_resource_gateway_url)\n",
    "    \n",
    "    utils.print_ok(f\"APIM Client Tool initialized successfully!\")\n",
    "    utils.print_info(f\"APIM Resource Name: {apim_resource_name}\")\n",
    "    utils.print_info(f\"APIM Gateway URL: {apim_resource_gateway_url}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    utils.print_error(f\"Error initializing APIM Client Tool: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b7e05",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2Ô∏è‚É£ Extract Current APIM Backend-Pools Configuration\n",
    "\n",
    "Retrieve and analyze the existing backend pools and backends configured in your APIM instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract current backends from APIM using the SDK\n",
    "utils.print_info(\"Extracting current APIM backends configuration...\")\n",
    "\n",
    "try:\n",
    "    # Use the APIMClientTool's new get_backends method (uses Azure SDK instead of CLI)\n",
    "    existing_backends, existing_backend_pools = apimClientTool.get_backends()\n",
    "    \n",
    "except Exception as e:\n",
    "    utils.print_error(f\"Error extracting backends: {e}\")\n",
    "    existing_backends = []\n",
    "    existing_backend_pools = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get supported models from the policy fragment (if exists)\n",
    "try:\n",
    "    supported_models_from_policy = apimClientTool.get_policy_fragment_supported_models(\"set-backend-pools\")\n",
    "    utils.print_ok(f\"Supported models in APIM policy fragment 'set-backend-pools':\")\n",
    "    for model in supported_models_from_policy:\n",
    "        print(f\"  ‚Ä¢ {model}\")\n",
    "except Exception as e:\n",
    "    utils.print_warning(f\"Could not retrieve policy fragment (may not exist yet): {e}\")\n",
    "    supported_models_from_policy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ad630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of current configuration\n",
    "utils.print_info(\"\\n\" + \"=\"*60)\n",
    "utils.print_info(\"CURRENT APIM BACKEND CONFIGURATION SUMMARY\")\n",
    "utils.print_info(\"=\"*60)\n",
    "\n",
    "if existing_backends:\n",
    "    print(\"\\nüìã Individual Backends:\")\n",
    "    for backend in existing_backends:\n",
    "        print(f\"  ‚Ä¢ {backend['name']}\")\n",
    "        print(f\"    URL: {backend['url']}\")\n",
    "        if backend['supportedModels']:\n",
    "            print(f\"    Models: {', '.join(backend['supportedModels'])}\")\n",
    "\n",
    "if existing_backend_pools:\n",
    "    print(\"\\nüì¶ Backend Pools:\")\n",
    "    for pool in existing_backend_pools:\n",
    "        print(f\"  ‚Ä¢ {pool['name']}\")\n",
    "        for svc in pool['services']:\n",
    "            print(f\"    - {svc.get('id', 'N/A')} (priority: {svc.get('priority', 'N/A')}, weight: {svc.get('weight', 'N/A')})\")\n",
    "\n",
    "if supported_models_from_policy:\n",
    "    print(f\"\\nü§ñ Total Supported Models: {len(supported_models_from_policy)}\")\n",
    "    print(f\"   {', '.join(supported_models_from_policy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f4643",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3Ô∏è‚É£ Discover Managed Identity for APIM Authentication\n",
    "\n",
    "Auto-discover or specify the user-assigned managed identity used by APIM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover managed identity from APIM using the SDK\n",
    "utils.print_info(\"Discovering managed identity configuration...\")\n",
    "\n",
    "# Use the APIMClientTool's get_managed_identity_info method\n",
    "managed_identity_info = apimClientTool.get_managed_identity_info()\n",
    "\n",
    "managed_identity_client_id = managed_identity_info.get('clientId')\n",
    "managed_identity_name = managed_identity_info.get('name') or apim_managed_identity_name\n",
    "managed_identity_resource_group = managed_identity_info.get('resourceGroup') or governance_hub_resource_group\n",
    "\n",
    "if not managed_identity_client_id:\n",
    "    utils.print_warning(\"Could not auto-discover managed identity. Please specify it manually in the configuration.\")\n",
    "else:\n",
    "    utils.print_info(f\"Client ID: {managed_identity_client_id}\")\n",
    "\n",
    "if managed_identity_name:\n",
    "    utils.print_ok(f\"Managed Identity Name: {managed_identity_name}\")\n",
    "    utils.print_ok(f\"Managed Identity Resource Group: {managed_identity_resource_group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58385417",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "### 4Ô∏è‚É£ Generate LLM Backend Parameter File\n",
    "\n",
    "Generate a customizable `.bicepparam` file with the full list of LLM backends to be integrated with APIM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LLM backends for deployment\n",
    "# You can modify the llm_backends_config list defined in the initialization cell\n",
    "\n",
    "utils.print_info(\"LLM Backends to be deployed:\")\n",
    "for backend in llm_backends_config:\n",
    "    print(f\"\\n  üîó {backend['backendId']}\")\n",
    "    print(f\"     Type: {backend['backendType']}\")\n",
    "    print(f\"     Endpoint: {backend['endpoint']}\")\n",
    "    print(f\"     Auth: {backend['authScheme']}\")\n",
    "    print(f\"     Models: {', '.join(backend['supportedModels'])}\")\n",
    "    print(f\"     Priority: {backend.get('priority', 1)}, Weight: {backend.get('weight', 100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the .bicepparam file content\n",
    "bicep_dir = \"../bicep/infra/llm-backend-onboarding\"\n",
    "params_file = os.path.join(bicep_dir, \"llm-backends-generated-local.bicepparam\")\n",
    "\n",
    "# Format backends array for Bicep\n",
    "def format_backend_for_bicep(backend):\n",
    "    models_str = \"\\n      \".join([f\"'{m}'\" for m in backend['supportedModels']])\n",
    "    return f\"\"\"  {{\n",
    "    backendId: '{backend['backendId']}'\n",
    "    backendType: '{backend['backendType']}'\n",
    "    endpoint: '{backend['endpoint']}'\n",
    "    authScheme: '{backend['authScheme']}'\n",
    "    supportedModels: [\n",
    "      {models_str}\n",
    "    ]\n",
    "    priority: {backend.get('priority', 1)}\n",
    "    weight: {backend.get('weight', 100)}\n",
    "  }}\"\"\"\n",
    "\n",
    "backends_bicep_str = \"\\n\".join([format_backend_for_bicep(b) for b in llm_backends_config])\n",
    "\n",
    "params_content = f\"\"\"using './main.bicep'\n",
    "\n",
    "// ============================================================================\n",
    "// LLM Backend Onboarding - Generated Parameter File\n",
    "// Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "// ============================================================================\n",
    "\n",
    "// ============================================================================\n",
    "// API Management (APIM) Configuration\n",
    "// ============================================================================\n",
    "param apim = {{\n",
    "  subscriptionId: '{subscription_id}'\n",
    "  resourceGroupName: '{governance_hub_resource_group}'\n",
    "  name: '{apim_resource_name}'\n",
    "}}\n",
    "\n",
    "// ============================================================================\n",
    "// APIM Managed Identity Configuration\n",
    "// ============================================================================\n",
    "param apimManagedIdentity = {{\n",
    "  subscriptionId: '{subscription_id}'\n",
    "  resourceGroupName: '{managed_identity_resource_group}'\n",
    "  name: '{managed_identity_name}'\n",
    "}}\n",
    "\n",
    "// ============================================================================\n",
    "// LLM Backend Configuration Array\n",
    "// ============================================================================\n",
    "param llmBackendConfig = [\n",
    "{backends_bicep_str}\n",
    "]\n",
    "\n",
    "// ============================================================================\n",
    "// Circuit Breaker Configuration\n",
    "// ============================================================================\n",
    "param configureCircuitBreaker = true\n",
    "\"\"\"\n",
    "\n",
    "# Write the parameter file\n",
    "utils.print_info(f\"Generating parameter file: {params_file}\")\n",
    "with open(params_file, 'w') as f:\n",
    "    f.write(params_content)\n",
    "\n",
    "utils.print_ok(f\"Parameter file generated successfully!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATED PARAMETER FILE CONTENT:\")\n",
    "print(\"=\"*60)\n",
    "print(params_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79744a37",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "### 5Ô∏è‚É£ Deploy LLM Backend Onboarding Bicep\n",
    "\n",
    "Deploy the LLM backends, backend pools, and policy fragments to APIM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00749dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the LLM backend onboarding\n",
    "deployment_name = f\"llm-backend-onboarding-{time.strftime('%Y%m%d%H%M%S')}\"\n",
    "template_file = os.path.join(bicep_dir, \"main.bicep\")\n",
    "\n",
    "utils.print_info(f\"Starting deployment: {deployment_name}\")\n",
    "utils.print_info(f\"Template: {template_file}\")\n",
    "utils.print_info(f\"Parameters: {params_file}\")\n",
    "\n",
    "# Run the subscription-level deployment\n",
    "deployment_cmd = f\"az deployment sub create --name {deployment_name} --location {location} --template-file {template_file} --parameters {params_file}\"\n",
    "\n",
    "output = utils.run(\n",
    "    deployment_cmd,\n",
    "    f\"Deployment '{deployment_name}' succeeded\",\n",
    "    f\"Deployment '{deployment_name}' failed\"\n",
    ")\n",
    "\n",
    "if output.success:\n",
    "    utils.print_ok(\"Deployment completed successfully!\")\n",
    "    \n",
    "    # Display deployment outputs if available\n",
    "    outputs = output.json_data.get('properties', {}).get('outputs', {}) if output.json_data else {}\n",
    "    \n",
    "    if outputs:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DEPLOYMENT OUTPUTS:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for key, value in outputs.items():\n",
    "            print(f\"  {key}: {value.get('value')}\")\n",
    "    else:\n",
    "        utils.print_info(\"No deployment outputs returned.\")\n",
    "else:\n",
    "    utils.print_error(\"Deployment failed. Check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5155939",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "### 6Ô∏è‚É£ Verify Deployed Configuration\n",
    "\n",
    "Verify that the backends, pools, and policy fragments were created successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c33691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize APIM client to pick up new backends\n",
    "apimClientTool.initialize()\n",
    "\n",
    "# Get updated supported models from policy fragment\n",
    "try:\n",
    "    updated_supported_models = apimClientTool.get_policy_fragment_supported_models(\"set-backend-pools\")\n",
    "    utils.print_ok(f\"Updated supported models in APIM policy fragment 'set-backend-pools':\")\n",
    "    for model in updated_supported_models:\n",
    "        print(f\"  ‚Ä¢ {model}\")\n",
    "except Exception as e:\n",
    "    utils.print_error(f\"Error retrieving policy fragment: {e}\")\n",
    "    updated_supported_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of current configuration\n",
    "utils.print_info(\"\\n\" + \"=\"*60)\n",
    "utils.print_info(\"CURRENT APIM BACKEND CONFIGURATION SUMMARY\")\n",
    "utils.print_info(\"=\"*60)\n",
    "\n",
    "if existing_backends:\n",
    "    print(\"\\nüìã Individual Backends:\")\n",
    "    for backend in existing_backends:\n",
    "        print(f\"  ‚Ä¢ {backend['name']}\")\n",
    "        print(f\"    URL: {backend['url']}\")\n",
    "        if backend['supportedModels']:\n",
    "            print(f\"    Models: {', '.join(backend['supportedModels'])}\")\n",
    "\n",
    "if existing_backend_pools:\n",
    "    print(\"\\nüì¶ Backend Pools:\")\n",
    "    for pool in existing_backend_pools:\n",
    "        print(f\"  ‚Ä¢ {pool['name']}\")\n",
    "        for svc in pool['services']:\n",
    "            print(f\"    - {svc.get('id', 'N/A')} (priority: {svc.get('priority', 'N/A')}, weight: {svc.get('weight', 'N/A')})\")\n",
    "\n",
    "if supported_models_from_policy:\n",
    "    print(f\"\\nü§ñ Total Supported Models: {len(supported_models_from_policy)}\")\n",
    "    print(f\"   {', '.join(supported_models_from_policy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665410dc",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Test Deployed Models\n",
    "\n",
    "The following sections test the deployed models through both the Universal LLM API and Azure OpenAI API endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542c43a",
   "metadata": {},
   "source": [
    "<a id='test-universal'></a>\n",
    "### üß™ Test via Universal LLM API (models/chat/completions)\n",
    "\n",
    "Test the deployed models using the Universal LLM API which routes based on the `model` field in the request body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7842c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover the Universal LLM API endpoint\n",
    "apimClientTool.discover_api(\"models\")\n",
    "azure_endpoint_models = str(apimClientTool.azure_endpoint)\n",
    "chat_completions_url_models = f\"{azure_endpoint_models}models/chat/completions?api-version={inference_api_version}\"\n",
    "\n",
    "utils.print_info(f\"Universal LLM API Endpoint: {chat_completions_url_models}\")\n",
    "\n",
    "# Get an API key from subscriptions\n",
    "if apimClientTool.apim_subscriptions:\n",
    "    api_key = apimClientTool.apim_subscriptions[-1].get(\"key\")\n",
    "    utils.print_ok(f\"Using subscription: {apimClientTool.apim_subscriptions[-1].get('name')}\")\n",
    "else:\n",
    "    utils.print_error(\"No APIM subscriptions found. Please create a subscription first.\")\n",
    "    api_key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603223fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each supported model via Universal LLM API\n",
    "if api_key and updated_supported_models:\n",
    "    utils.print_info(f\"\\nTesting {len(updated_supported_models)} models via Universal LLM API...\\n\")\n",
    "    \n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is 2+2? Answer in one word.\"}\n",
    "    ]\n",
    "    \n",
    "    for model_name in updated_supported_models[:3]:  # Test first 3 models\n",
    "        utils.print_info(f\"Testing model: {model_name}\")\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": test_messages\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                chat_completions_url_models,\n",
    "                headers={\"api-key\": api_key},\n",
    "                json=payload,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            utils.print_response_code(response)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                answer = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response\")\n",
    "                region = response.headers.get(\"x-ms-region\", \"unknown\")\n",
    "                print(f\"  üí¨ Response: {answer}\")\n",
    "                print(f\"  üìç Backend Region: {region}\")\n",
    "                utils.print_ok(f\"Model '{model_name}' - SUCCESS\\n\")\n",
    "            else:\n",
    "                utils.print_error(f\"Model '{model_name}' - FAILED: {response.text}\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            utils.print_error(f\"Model '{model_name}' - ERROR: {str(e)}\\n\")\n",
    "else:\n",
    "    utils.print_warning(\"Cannot run tests - missing API key or supported models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f16b0",
   "metadata": {},
   "source": [
    "<a id='test-openai'></a>\n",
    "### üß™ Test via Azure OpenAI API (openai/deployments/{model}/chat/completions)\n",
    "\n",
    "Test the deployed models using the Azure OpenAI compatible API which uses the deployment name in the URL path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc65b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover the Azure OpenAI API endpoint\n",
    "try:\n",
    "    apimClientTool.discover_api(\"openai\")\n",
    "    azure_endpoint_openai = str(apimClientTool.azure_endpoint)\n",
    "    utils.print_info(f\"Azure OpenAI API Base Endpoint: {azure_endpoint_openai}\")\n",
    "except Exception as e:\n",
    "    utils.print_warning(f\"Azure OpenAI API not found in APIM: {e}\")\n",
    "    azure_endpoint_openai = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models via Azure OpenAI API format\n",
    "if api_key and azure_endpoint_openai and updated_supported_models:\n",
    "    utils.print_info(f\"\\nTesting models via Azure OpenAI API format...\\n\")\n",
    "    \n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}\n",
    "    ]\n",
    "    \n",
    "    for model_name in updated_supported_models[:3]:  # Test first 3 models\n",
    "        utils.print_info(f\"Testing model: {model_name}\")\n",
    "        \n",
    "        # Azure OpenAI format uses deployment name in URL path\n",
    "        chat_completions_url_openai = f\"{azure_endpoint_openai}openai/deployments/{model_name}/chat/completions?api-version={inference_api_version}\"\n",
    "        \n",
    "        payload = {\n",
    "            \"messages\": test_messages  # No model field needed - it's in the URL\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                chat_completions_url_openai,\n",
    "                headers={\"api-key\": api_key},\n",
    "                json=payload,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            utils.print_response_code(response)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                answer = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response\")\n",
    "                region = response.headers.get(\"x-ms-region\", \"unknown\")\n",
    "                print(f\"  üí¨ Response: {answer}\")\n",
    "                print(f\"  üìç Backend Region: {region}\")\n",
    "                utils.print_ok(f\"Model '{model_name}' - SUCCESS\\n\")\n",
    "            else:\n",
    "                utils.print_error(f\"Model '{model_name}' - FAILED: {response.text}\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            utils.print_error(f\"Model '{model_name}' - ERROR: {str(e)}\\n\")\n",
    "else:\n",
    "    utils.print_warning(\"Cannot run Azure OpenAI API tests - missing API key, endpoint, or supported models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdf14e",
   "metadata": {},
   "source": [
    "<a id='test-sdk'></a>\n",
    "### üß™ Test using Azure OpenAI Python SDK\n",
    "\n",
    "Test using the official Azure OpenAI Python SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63338618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "if api_key and azure_endpoint_openai and updated_supported_models:\n",
    "    model_name = updated_supported_models[0]  # Use first available model\n",
    "    utils.print_info(f\"Testing with Azure OpenAI SDK using model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=azure_endpoint_openai,\n",
    "            api_key=api_key,\n",
    "            api_version=inference_api_version\n",
    "        )\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Say 'Hello from Azure OpenAI SDK!'\"}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        utils.print_ok(\"SDK Test Successful!\")\n",
    "        print(f\"üí¨ Response: {response.choices[0].message.content}\")\n",
    "        print(f\"üìä Usage: {response.usage.total_tokens} tokens\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        utils.print_error(f\"SDK Test Failed: {str(e)}\")\n",
    "else:\n",
    "    utils.print_warning(\"Cannot run SDK test - missing prerequisites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d9e66",
   "metadata": {},
   "source": [
    "<a id='test-streaming'></a>\n",
    "### üß™ Test Streaming Response\n",
    "\n",
    "Test streaming responses using the Azure OpenAI SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f192e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "if api_key and azure_endpoint_openai and updated_supported_models:\n",
    "    model_name = updated_supported_models[0]  # Use first available model\n",
    "    utils.print_info(f\"Testing streaming with model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=azure_endpoint_openai,\n",
    "            api_key=api_key,\n",
    "            api_version=inference_api_version\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = client.chat.completions.with_raw_response.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Count from 1 to 10 with commas between each number.\"}\n",
    "            ],\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        print(f\"üì° x-ms-region: {response.headers.get('x-ms-region', 'unknown')}\")\n",
    "        print(f\"üì° x-ms-stream: {response.headers.get('x-ms-stream', 'N/A')}\")\n",
    "        print(\"\\nüí¨ Streaming response:\")\n",
    "        \n",
    "        completion = response.parse()\n",
    "        collected_content = []\n",
    "        \n",
    "        for chunk in completion:\n",
    "            if chunk.choices and chunk.choices[0].delta.content:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                collected_content.append(content)\n",
    "                print(content, end='', flush=True)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n\\n‚úÖ Stream completed in {elapsed:.2f} seconds\")\n",
    "        print(f\"üìù Full response: {''.join(collected_content)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        utils.print_error(f\"Streaming Test Failed: {str(e)}\")\n",
    "else:\n",
    "    utils.print_warning(\"Cannot run streaming test - missing prerequisites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa78350",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Summary\n",
    "\n",
    "This notebook completed the following tasks:\n",
    "\n",
    "1. ‚úÖ **Extracted** current APIM backend-pools configurations\n",
    "2. ‚úÖ **Generated** a customizable LLM backend parameter file (`.bicepparam`)\n",
    "3. ‚úÖ **Deployed** the LLM onboarding Bicep templates\n",
    "4. ‚úÖ **Tested** the deployed models through:\n",
    "   - Universal LLM API (`/models/chat/completions`)\n",
    "   - Azure OpenAI API (`/openai/deployments/{model}/chat/completions`)\n",
    "   - Azure OpenAI Python SDK\n",
    "   - Streaming responses\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Modify the `llm_backends_config` in the initialization cell to add more backends\n",
    "- Re-run the deployment cells to update the APIM configuration\n",
    "- Use the generated parameter file as a template for CI/CD pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
